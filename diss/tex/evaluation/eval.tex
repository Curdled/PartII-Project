% \documentclass[../../diss.tex]{subfiles}
\documentclass[float=false, crop=false]{standalone}
\usepackage[subpreambles=true]{standalone}
\usepackage{import}

\usepackage{subfiles}
\usepackage[backend=biber,style=numeric,sorting=none]{biblatex}
\usepackage[pdfborder={0 0 0}]{hyperref}    % turns references into hyperlinks
\usepackage[margin=25mm]{geometry}  % adjusts page layout
\usepackage{graphicx}  % allows inclusion of PDF, PNG and JPG images
\usepackage{docmute}   % only needed to allow inclusion of proposal.tex
\usepackage[utf8]{inputenc}
\usepackage{textcomp}
\usepackage{import}
\usepackage[font=small,labelfont=bf]{caption}

\usepackage{wrapfig,lipsum,booktabs}


\usepackage{pgfplots}
\pgfplotsset{compat=newest}
%% the following commands are needed for some matlab2tikz features
\usetikzlibrary{plotmarks}
\usetikzlibrary{arrows.meta}
\usepgfplotslibrary{patchplots}
\usepackage{grffile}
\usepackage{amsmath}

\usepackage{verbatim}

\usepackage{csquotes}
\usepackage{listings}

\definecolor{pblue}{rgb}{0.13,0.13,1}
\definecolor{pgreen}{rgb}{0,0.5,0}
\definecolor{pred}{rgb}{0.9,0,0}
\definecolor{pgrey}{rgb}{0.46,0.45,0.48}

\newlength\gwidth
\newlength\gheight
\setlength{\gwidth}{\textwidth}
\setlength{\gwidth}{\textheight}

\usepackage[capitalize, nameinlink]{cleveref}
\crefdefaultlabelformat{#2\textbf{#1}#3}

\crefname{figure}{\textbf{Figure}}{\textbf{Figures}}
\crefname{table}{\textbf{Table}}{\textbf{Tables}}
\crefname{appendix}{\textbf{Appendix}}{\textbf{Appendices}}

\newcommand{\importMGraph}[3]{\setlength{\gwidth}{#2}\setlength{\gheight}{#3}\subimport{graphs/}{#1}}

\lstdefinestyle{HaskellStyle}{
  frame=none,
  xleftmargin=2pt,
  stepnumber=1,
  numbersep=5pt,
  numberstyle=\ttfamily\tiny\color[gray]{0.3},
  belowcaptionskip=\bigskipamount,
  commentstyle=\color{pgreen},
  keywordstyle=\color{pblue},
  stringstyle=\color{pred},
  basicstyle=\ttfamily,
  captionpos=b,
  escapeinside={&}{&},
  language=haskell,
  tabsize=2,
  emphstyle={\bf},
  commentstyle=\it,
  showspaces=false,
  columns=flexible,
  showstringspaces=false,
  morecomment=[l]
}

\lstnewenvironment{JavaLst}
{\lstset{language=Java,
  showspaces=false,
  showtabs=false,
  breaklines=true,
  showstringspaces=false,
  breakatwhitespace=true,
  commentstyle=\color{pgreen},
  keywordstyle=\color{pblue},
  stringstyle=\color{pred},
  basicstyle=\ttfamily,
  escapeinside={&}{&},
  moredelim=[il][\textcolor{pgrey}]{$$},
  moredelim=[is][\textcolor{pgrey}]{\%\%}{\%\%}
}}{}

\lstnewenvironment{HaskellLst}{%
  \lstset{style=HaskellStyle}}{}


\begin{document}
The success criteria for this project fall into three categories:
\begin{itemize}
  \item Correctness.
  \item Performance.
  \item Usability.
\end{itemize}
I will split the evaluation into three parts.

% , first we will
% look at the correctness of the overall project. This was established 
% with unit tests of every stage of the compiler in isolation. I 
% have justified certain choices formally (the mapping from a lazy to eager semantics
% shown in Appendix~\ref{appendix:bytecodetranslationdoc}). The performance evaluation follows 
% with comparisons between JVHC and other already existing compilers such as
% GHC, there is also a comparison of functional inlining. Usability is then discussed in more detail.

\section{Correctness}

There are unit tests for all stages of the compiler, these were implemented 
using the Haskell library HUnit \cite{hunit-lib}. The test framework for code generation
was written by me to check:
\begin{itemize}
  \item the returned value was identical to the output expected. 
  \item that anything printed to \texttt{stdout} was the correct value in the correct order. 
  \item the program terminated if it was expected to.
\end{itemize}
  The framework to test code generation compiles a given program to a set 
  of \texttt{.class} files, then 
  starts an instance of the JVM which will execute these class files
  checking that all the above conditions hold.

  I justified the translation from \verb|CoreExpr| to JVM bytecode
  formally (the mapping from a lazy to eager
  semantics shown in \cref{appendix:bytecodetranslationdoc}).

\section{Performance}

\subsection{Methodology}

Haskell being a lazy language, builds up expressions and only
evaluates them when required. 
This can be a problem for benchmarking since timing how long a
computation takes it not a straight forward as just executing that line
of code finding the different between the system time before and after the line.
The expression may not have been evaluated fully. 
To formalise this define \textit{weak head normal form} (WHNF) and 
\textit{normal form} (NF) as:
\begin{displayquote}
  An expression is in weak head normal form, if it is either:
  \begin{itemize}
    \item A data constructor.

    \item A built-in function applied to too few arguments.

    \item A lambda abstraction \verb|\x -> expression|.
  \end{itemize}
\end{displayquote}
  \begin{flushright} 
    Definition from Haskell wiki \cite{haskell-whnf}.
  \end{flushright}
  \begin{displayquote}
    An expression is in normal form, if it contains no $\beta$-redexes. 
    A $\beta$-redex is an expression of the form $(\lambda x.e)e'$, 
    this is a possible application that has not been reduced.
  \end{displayquote}
Then when benchmarking we want all expression to be reduced to \textit{normal form}. If expressions
are reduced to normal form then the timing will show how long it takes to evaluate the expression
without the lazy nature of Haskell. 
The Haskell report defines a function called \verb|seq| which is used to 
reduce an expression to WHNF, this is not sufficient for benchmarking. 
GHC provides a function called \verb|deepseq| to reduce expression to NF.
Using \verb|deepseq| when reducing an expression will result in the 
correct value when timing a function.
When taking timing measurements of outputted code running on the JVM care
must be taken to produce repeatable results. The JVM will optimize loaded
in bytecode finding common control flow paths and optimizing the translated
bytecode for this path. After multiple executions
of the same \verb|.class| file the JVM will compile the bytecode
into native code for the architecture that the JVM is running on. 
The native compilation is slower than interpretation if the file
is only run a few times, since the compilation takes longer than
interpreting the \verb|.class| file these few times.
To reduce the variation is timings, the JVM will be started up for 
each individual timing test. The timing code is embedded into the
generated program. The timing just finds the difference in 
system clock before and after running the main method. The 
system clock value is collected using the system call
\verb|java/lang/System/currentTimeMillis:()J|.


\subsection{Results}

I will look at the following performance metrics and will compare with other compilers where
appropriate:

\begin{itemize}
  \item Compile time.
  \item Type inference time.
  \item Code size.
  \item Program runtime.
  \item Call stack height.
  \item Memory usage.
  \item Garbage collection times.
  \item Comparison of inlining.
\end{itemize}


\begin{wrapfigure}{r}{11.5cm}
  \centering
      \importMGraph{plotStage.tex}{0.7\textwidth}{0.22\textwidth}
      \caption{Average time taken at each compiler stage to compile an 
      average program}
      \label{plot:TimeInStages}
\end{wrapfigure}
When a user first interacts with JVHC, they will notice the time 
taken to compile a program, so the performance evaluation will begin from there,
the results can be seen in \cref{plot:TimeInStages}. 
This shows that the code generation is by far the most time 
consuming part of the pipeline. 
There is then a comparison of type inference between GHC and JVHC 
showing only a two times decrease in the runtime cost of JVHC's TI 
algorithm in \cref{plot:tiTime}. 
To show that the implementation of
type inference algorithm for the \verb|tiExpr| program is exponential
more data points would be needed.
Running the TI algorithm for values of $n$ larger than 5,
caused problems since it used all the memory available to the system.


\begin{figure}
  \begin{samepage}
  \importMGraph{tiTime.tex}{0.93\textwidth}{0.4\textwidth}
  \caption[Type inference runtime as a function of input size]
  {Plot of number of pairs size (increasing $n$ in \texttt{f$n$}) 
    of the tiExpr program in \cref{appendix:programs} against time taken 
    to run type inference algorithm. The data has been fitted to a line of the form
    $y = a \cdot e^{b\cdot x}+c$, where $a,b,c$ and parameters.}
  \label{plot:tiTime}
\end{samepage}
\end{figure}
There will be three programs explicitly referenced throughout the evaluation section 
[\texttt{fib}, \texttt{testN}, \texttt{badProg}] they are all defined in 
\cref{appendix:programs}.
Outputted code size is a good measure of how efficiently a program can be compiled into an executable format. 
The comparison is not completely fair between JVHC, Eta and GHC 
since the latter two will use dead code elimination. 
The results are in \cref{plot:codeSize}. \texttt{-O0} (No optimizations flag) 
was used when compiling, however the GHC flag guide \cite{haskell-flag-ref}
does not provide a way to disable what it calls \texttt{-fwarn-unused-binds}.
It can be seen that inlined code has a slightly higher code size than non inlined code. This extra code size is due to having both the inlined and 
non-inlined version of a function compiled to bytecode. 
The problem could be solved with dead code elimination. GHC has a much larger 
and more varied code size than all other compilers since this must produce a stand alone binary 
whereas Eta, and JVHC run on the JVM with extra support libraries (not included in this comparison) 
and the garbage collection logic is in the JVM whereas GHC must embed the logic in the binary. 
It can be seen from \cref{table:codeSize}. 
that Eta has the smallest code size increase. The variation of GHC is so large I have not
included straight line fit coefficients for GHC. The data in the table
was generated using the Matlab function \verb|polyfit(x,y,1)|.


\begin{figure}
  \centering
  \importMGraph{plotCodeSize.tex}{0.90\textwidth}{0.4\textwidth}
  \caption[Code size as a function of program input size in characters]
          {Code size as a function of program input size in characters. Note that 
           there are two axes, one in kB and the other in MB (for GHC). The right axis 
           doesn't start at 0.}
  \label{plot:codeSize}
\end{figure}

\begin{wraptable}{r}{7.5cm}
  \centering
  \begin{tabular}{ c | c c }
    Compiler& $a$ & $b$\\
    \hline
    JVHC & $39.9$ & $2.47 \times 10^{4}$ \\
    JHVC with inlining & $44.7$ & $2.58 \times 10^{4}$\\
    eta & $6.74$ & $1.45 \times 10^{4}$
  \end{tabular}
  \caption{Regression values for code size form data in \cref{plot:codeSize} to $y=a x+b$}
  \label{table:codeSize}
\end{wraptable}

\cref{plot:fibTiming} contains a comparison of the runtime of the \texttt{fib}$n$ program where 
$n$ ranges between $[1,35]$ inclusive, showing that the asymptotic runtime of the code output
by each compiler has the same asymptotic complexity.
  Least squares regression was carried out where the data was fitted to the curve
  $y = a \cdot \varphi^n$ (using the \texttt{lsqcurvefit} Matlab function).
  \cref{table:fibCoef} shows
  JVHC is approximately $10^2$  times slower than the native
  Java program and only approximately 1.2 times slower than the GHC 
  compiled program.
  This data was obtained using \texttt{-O0} flag on GHC and no inlining
  in JVHC.

\begin{figure}
  \centering
  \importMGraph{plotFibRuntime}{0.93\textwidth}{0.4\textwidth}
  \caption{Comparison between runtime for finding the $n$th Fibonacci number na\"ively as a 
  function of the $n$, note the double axis.}
  \label{plot:fibTiming}
\end{figure}

\begin{wraptable}{r}{7.5cm}
  \centering
\begin{tabular}{c | c}
  & $a$\\
  \hline 
  JVHC & $1.13\times 10^{-7}$\\
  GHC & $9.49\times 10^{-8}$\\
  Java & $1.62 \times 10^{-9}$
\end{tabular}
\caption{Coefficients from fitting the data in \cref{plot:fibTiming} 
    to the line $y=a\cdot \varphi^n$}
  \label{table:fibCoef}
\end{wraptable}

The effect that inlining has on the runtime of some program can be seen from
\cref{plot:inlinePlot}. 
 There is a noticeable runtime improvement when inlining is used.
Fitting both the sets of data to a straight line $y=ax+b$ yields the following
coefficients in \cref{table:inlineCoef}. Since the runtime of the program without inlining
have a discontinuity in the data gathered (when compared to a straight line), two straight lines has been
fit to the data. Part I is fit to the first six data points and Part II is fit to remaining data points.
\cref{table:inlineCoef} shows a twelve times speed up Part II of the curve.
  The discontinuity in runtime for the inlined program for $n > 2.7 \times 10^{4}$, 
  this is due to a single garbage collection cycle.
  This figure shows that there is a definite benefit, 
  in terms of runtime gained from applying inlining to \texttt{testN}. Inlining however
  will not always improve runtime performance, as will be shown later. 
\begin{figure}
\begin{samepage}
  \centering
  \importMGraph{inlinePlotComplex}{0.96\textwidth}{0.4\textwidth}
  \caption[Runtime of \texttt{testN} as a function on input size $n$]
  {Plot of input value passed to \texttt{testN} against time to run in seconds,
    also included in the time that the garbage collection (GC) ran in the program 
    without functional inlining, GC never ran during the execution of the 
    program with functional inlining.}
  \label{plot:inlinePlot}
\end{samepage}
\end{figure}

\begin{table}
  \centering
\begin{tabular}{c | c  c}
  & $a$ & $b$\\
  \hline 
  No Inlining (Part I) & $3.65\times 10^{-6}$ & $6.68 \times 10^{-2}$\\
  No Inlining (Part II) & $1.36\times 10^{-5}$ & $3.55 \times 10^{-2}$\\
  Inlining & $1.11\times 10^{-6}$ & $0.076$
\end{tabular}
\caption{Coefficients from fitting the data in \cref{plot:inlinePlot} to 
  a straight line $y=ax+b$}
  \label{table:inlineCoef}
\end{table}

Factors that effect the qualify of a compiled program include how much memory
is used throughout execution of a program. Two peformance metrics were collection throughout
separate runs of the program \texttt{testN}. The maximum call stack size
shown in \cref{plot:stackSize} and the maximum 
memory usage in \cref{plot:memUsed}. 
The maximum call stack height linearly increases with $n$, fitting to a straight line curve
with the equation $y = a\cdot x + b$ resulting in the table in \cref{table:stackGrowth} 
and about a two times reduction maximum call stack height. 
\begin{wraptable}{r}{7.5cm}
  \centering
  \begin{tabular}{ c | c c}
  & $a$ & $b$\\
  \hline
  No Inlining & 11 & 29 \\
  Inlining & 5 & 18 
  \end{tabular}
  \caption{Fitting of the data from \cref{plot:stackSize}  
    to the line $y = a\cdot x + b$.}
  \label{table:stackGrowth}
\end{wraptable}
There is also a noticeable different is peak memory usage, 
showing reduced memory usage when inlining is enabled.
There is a very high correlation between maximum stack height and memory usage.
Leading to the conclusion that reducing the max stack height would 
reduce memory usage, and if fewer objects are allocated, fewer garbage collection cycles would be
run throughout program execution.
Max stack height data point where collected by generating a call stack at any 
point in the computation which could be at maximum memory consumption. This is when 
the number of thunks allocated will be at a maximum.
The computation could be at maximum thunk depth at any leaf in the \texttt{CoreExpr} tree
(\texttt{Var} or \texttt{Lit}), this is when no new thunks will be allocated, before at least one thunk
value is computed. By default the maximum height that a stack trace can be is 1024 frames. 
The JVM flag \texttt{MaxJavaStackTraceDepth} was used to increase
the stack height to about $10^{4}$ which was enough for the programs being considered.
The data for memory usage was gathered by looking at JVM traces, 
enabled with the flag \texttt{PrintGCDetails}. 
This is therefore an over estimate of peak memory usage. 

\begin{figure}
  \centering
  \importMGraph{plotCallStackGrowth}{0.96\textwidth}{0.3\textwidth}
  \caption{Maximum call stack height for \texttt{testN} as a function of $n$}
  \label{plot:stackSize}
\end{figure}

\begin{figure}
  \centering
  \importMGraph{memUsed}{0.96\textwidth}{0.3\textwidth}
  \caption{Memory used in \texttt{testN} as a function of $n$}
  \label{plot:memUsed}
\end{figure}
Function inlining, as mentioned earlier, is not always a good idea.
It is possible that inlining functions may lead to increased runtime, shown  
\cref{plot:badInline}. 
The increase in runtime is due to inlining the expensive to compute function 
\texttt{sumN} which will be evaulated three times adding the results each time. 
Whereas no inlining will only compute \texttt{sumN} once and then add the result three times, 
which will run faster. The rise in the inlined program performance at about $120 \times 10^3$, 
could be due to the JVM optimizing the bytecode when it notices common execution patterns.
The heuristics used by the inlining compiler stage are very simple it would be possible to 
come up with better heuristics that can be used to decide whether to inline a function.

\begin{figure}
  \centering
  \importMGraph{plotBadInline}{0.96\textwidth}{0.3\textwidth}
  \caption{\texttt{badProg} as a function of $n$. Comparing the inlined and non-inlined
  versions of the compiled program}
    \label{plot:badInline}
\end{figure}

\section{Usability}
% There are three  the on the programmer that this compiler introduces:

This compiler introduces the following three burdens on the programmer:

\begin{itemize}
  \item Exposing Java functions in to the Haskell world.

  \item Exposing Java functions to the JVHC compiler, by passing type information and the name 
    of the function.

  \item Exposing Java programs to generated JVM bytecode.
\end{itemize}

\subsection{Exposing Java functions to the Haskell world}

Usability is hard to measure analytically, but an approximation that can be used is extra code size
(implementing the program in Haskell as compared to Java).
The code required to expose integer plus to the JVHC compiler. 
This can definitely be reduced, if there was time available in the project,
I would implement a code generation feature allowing plus to be defined
without the need for the boiler plate code.

\begin{JavaLst}
public class plus0 implements Function<Supplier<Integer>,plus1>, 
                              Supplier<plus0> {
    @Override
    public plus1 apply(Supplier<Integer> integer) 
      { return new plus1(integer); }
    @Override
    public plus0 get() { return new plus0(); }
}

public class plus1 implements Function<Supplier<Integer>,Integer>{
    Supplier<Integer> i0;
    plus1(Supplier<Integer> i0) { this.i0 = i0; }
    @Override
    public Integer apply(Supplier<Integer> i1) 
      { return i0.get() &\textcolor{pgreen}{+}& i1.get(); }
}
\end{JavaLst}

\subsection{Exposing Java functions to the JVHC compiler}

Currently there is a list of all the custom types defined for the compilation stage,
instead this should be moved to a separate definition file, or better still 
add new syntax to the Haskell like language accepted by JVHC allowing new type
definitions to be included.

\begin{HaskellLst}
foreign import java "add" add :: Int -> Int -> Int
\end{HaskellLst}
This syntax is a slightly modified declaration used in the foreign function interface (FFI) \cite{ghc_ffi} 
in GHC. The FFI is used to specify the name and type of C functions which can be called from 
instead the GHC compiled Haskell.

\subsection{Exposing Java programs to generated JVM bytecode}

The current Java interface produced by JVHC is harder to use than it could be
\begin{enumerate}
  \item Every argument passed into each function must first be wrapped in a thunk. 
  \item Generic information (defined shortly) is not added to the bytecode meaning that
the return values from function calls (\texttt{apply}) and thunk evaluation 
(\texttt{get}) require casting in Java source code, this means the programmer
must know the return type of each function and thunk. This will
make mixing these \verb|.class| files with Java code harder than need be.
\end{enumerate}

The first of these problems could be solved by generating a function $f_J$ for each 
function $f$ exposed from Haskell. Then $f_J$ will uncurry
(go from \mbox{$\tau_1 \rightarrow \dotsc \rightarrow \tau_n \rightarrow \tau$} to
\mbox{$(\tau_1,\dotsc,\tau_n) \rightarrow \tau$}) the arguments of the function, then 
wrap then values passed as argument to $f_J$ in thunks, and finally call the function $f$. 
Uncurrying is done to fit with the idiom in Java of write functions that take 
$n$-tuple instead of a applying the $n$ arguments to a set of $n$ curried functions
in one by one. 

Generic types are not directly supported by JVM bytecode for example
\verb|List<Intger>| (a list only containing objects of \verb|Integer| or a subtype)
will undergo type erasure to give \verb|List|. The \verb|List| 
will have a collection of objects of type \verb|java/lang/object| in an implementation
specific list structure. 
Generics exist at compile time (they are used by the compiler to enforce 
certain correctness properties). Generics play no role in execution at runtime
on the JVM. However it is possible to store generic type information
in JVM bytecode, this will be ignored by the JVM, but can be used by tools
which inspect bytecode and also by the Java compiler when trying to compile
Java code that references this bytecode.

Then a solution to problem (2) would be keeping generic information so it 
could be used by the Java compiler \texttt{javac} infer a return type of 
each \texttt{apply} and \texttt{get} which is closer to the true 
values of the return type (still might not be the correct type 
since Java allows sub-typing). 
This could be achieved by modifying the Codec-JVM library 
\cite{codec-jvm-link} adding support for tracking of generic types and then 
writing these generic types to the \textit{signature} data structure in 
\verb|.class| files. \textit{Signatures} defined in the JVM spec
\cite[\textsection4.7.9]{jvm-spec8} and is used to 
\begin{displayquote}
  records generic signature information for any class, interface, 
  constructor or member whose generic signature in the Java 
  programming language would include references to type variables 
  or parameterized types.
\end{displayquote}
\begin{flushright}
Exert from the JVM specification cited just above (\textsection4.7.9).
\end{flushright}

\end{document}
