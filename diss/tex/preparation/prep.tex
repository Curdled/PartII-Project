\documentclass[float=false, crop=false]{standalone}
\usepackage[subpreambles=true]{standalone}
\usepackage{import}

\usepackage{subfiles}
\usepackage[backend=biber,style=numeric,sorting=none]{biblatex}
\usepackage[pdfborder={0 0 0}]{hyperref}    % turns references into hyperlinks
\usepackage[margin=25mm]{geometry}  % adjusts page layout
\usepackage{graphicx}  % allows inclusion of PDF, PNG and JPG images
\usepackage[utf8]{inputenc}
\usepackage{textcomp}
\usepackage{import}
\usepackage[font=small,labelfont=bf]{caption}

\usepackage{amsmath}

\usepackage{csquotes}
\usepackage{color}

\usepackage{verbatim}

\newlength\gwidth
\newlength\gheight
\setlength{\gwidth}{\textwidth}
\setlength{\gwidth}{\textheight}

\newcommand{\namefig}{\textbf{Figure}~}

\renewcommand{\rmdefault}{bch} % change default font

\usepackage[english]{babel}
\usepackage[utf8]{inputenc}
\usepackage{tikz} 
\usetikzlibrary{arrows,decorations.pathmorphing,backgrounds,fit,positioning,shapes.symbols,chains}

\definecolor{pblue}{rgb}{0.13,0.13,1}
\definecolor{pgreen}{rgb}{0,0.5,0}
\definecolor{pred}{rgb}{0.9,0,0}
\definecolor{pgrey}{rgb}{0.46,0.45,0.48}

\usepackage{listings}
\lstnewenvironment{JavaLst}
{\lstset{language=Java,
  showspaces=false,
  showtabs=false,
  breaklines=true,
  showstringspaces=false,
  breakatwhitespace=true,
  commentstyle=\color{pgreen},
  keywordstyle=\color{pblue},
  stringstyle=\color{pred},
  basicstyle=\ttfamily,
  moredelim=[il][\textcolor{pgrey}]{$$},
  moredelim=[is][\textcolor{pgrey}]{\%\%}{\%\%}
}}{}

\lstnewenvironment{HaskellLst}
{\lstset{
  frame=none,
  xleftmargin=2pt,
  stepnumber=1,
  numbersep=5pt,
  numberstyle=\ttfamily\tiny\color[gray]{0.3},
  belowcaptionskip=\bigskipamount,
  commentstyle=\color{pgreen},
  keywordstyle=\color{pblue},
  stringstyle=\color{pred},
  basicstyle=\ttfamily,
  captionpos=b,
  escapeinside={*'}{'*},
  language=haskell,
  tabsize=2,
  emphstyle={\bf},
  commentstyle=\it,
  showspaces=false,
  columns=flexible,
  showstringspaces=false,
  morecomment=[l]\%,
}} {}

\lstnewenvironment{JVMLst}
{\lstset{
  frame=none,
  numbers=none,
  xleftmargin=2pt,
  language=JVMIS,
  emphstyle={\bf},
  stringstyle=\mdseries\rmfamily,
  commentstyle=\color{pgreen},
  stringstyle=\color{pred},
  keywordstyle=\color{black},
  basicstyle=\ttfamily,
  basicstyle=\small\sffamily
}} {}


\begin{document}

\section{Starting Point}

% Maybe look at a specific use case.
% Define success more clearly.
% Maybe move version control and backup to separate section.

  I have taken the Compiler Construction course in Part IB and am currently taking the Part II course Types. I have spent the summer
  learning Haskell from various places including CIS 194 University of Pennsylvania \cite{cis194} and Real World Haskell \cite{realworldhaskell}.
  I plan to use Alex and Happy, which are lexer and parser generators
  respectively, to implement the first stage in the pipeline.
  I plan to use a library called \textit{codec-jvm} to generate JVM bytecode from an expression I will build up from the core language generated after
  syntactic desugaring. The \textit{codec-jvm} is a library that will allow me to build up a typed expression which will map directly
  to JVM bytecode. However, this library is just an assembler, I will still need to generate code to implement all of the
  programming language constructs in JVM bytecode myself.

\section{Requirements Analysis}

The layout of the project divided itself cleanly into five stages, the five stages are
\begin{enumerate}
  \item Lexing and Parsing.
  \item Desugaring.
  \item Type Inference.
  \item Optimisation.
  \item Code Generation.
\end{enumerate}

\begin{figure}
  \centering
  \subimport{stageDiagram/}{stageDiagram.tex}
  \caption{The five stages of the compiler pipeline.}
\end{figure}
Each of these stages could be thought about in separation, therefore I will actively 
reduce code sharing between the pipeline stages and only have pipeline stages dependent 
on the previous stage, this will help reduce complexity between the stages and 
allow development of stages without relying on future 
stages. I will use a waterfall model, where I implement a stage
at a time in the order given above, while writing tests to help check
correctness of implementation. Since each stage of the pipeline
is isolated unit test can be written checking each stage separately, this means 
I can be sure that a given stage will operation on data correctly before considering
a different stage.
I will in turn look at specific preparations made for each stage in the pipeline.
Type classes are a core element of the Haskell language, I didn't include Type classes
in the JVHC implementation of Haskell, since I already had plenty of work packages in
the project schedule. Without type classes I still believe I can show all
the same skills with or without implementing type classes, but with a more manageable work schedule.

\section{Success Criteria}

The success criteria have changed very little since they were outlined in
Appendix~\ref{appendix:proposal}. 
The success criteria can be summarised as testing for correctness
with the aid of unit tests. The work in all of the work package of each fall into
one of the following major piece of the compiler that must be implemented
[Lexing, Parsing, Desugaring, Type Inference,
Code Generation, Optermization, Expose Java functions to Haskell,
Support for I/O (via IO Monad)]. 
Then the extra success criteria are that the solution should 
comparable in runtime performance to existing Haskell implementations 
such as GHC. 
Then the success criteria can be extended still further again, I will
pay some attention to the resulting interface provided by the generated
code which is exposed to other language on the JVM, I will look
at the Java interface.

\section{Tools Used}

There were many tools used throughout the project, git was used for 
code management, version control and new idea exploration. Every time
I had a new idea about how to implement something I could create new a 
branch, work on the feature, then if the implementation idea was successful, 
it would be merged back into the main branch. 
The git history was used when writing the dissertation.
I used commit history to create the time line
of implementation work activity. 
I created a Github repository and used this to backup milestones reached 
along the project using the \verb|git push| command.
Dropbox was used through the project to backup code files and data generated 
through the project, Dropbox will automatically sync and file change
onto the external Dropbox servers. Dropbox also has a record of all
version of a synced file, I sometimes needed access to old versions
of data files, this Dropbox feature was invaluable throughout the project.
IntelliJ \cite{intellij-ide} was used as an IDE when writing the Java
runtime library associated with JVHC. IntelliJ was also used to decompile
bytecode generated from JVHC, this was very useful when checking logical
bugs in the outputted bytecode.
Haskell's default build system is called Cabal \cite{cabal}. I 
decided to since Cabal since there would be multiple included external libraries,
many code files and different project configurations.
A build system would therefore be very useful to manage this
complexity. Cabal is very simple tool to use and provides support
for many different project types:
  \begin{itemize}
    \item Library (This is how the compiler code is implemented).

    \item Executable (This is how the front end part of the compiler is 
      implemented).

    \item Test.

    \item Benchmark.
  \end{itemize}
For writing unit tests I am using HUnit \cite{hunit},
this provides a unit test framework and also a quick-check framework. 
I will use the unit test framework of the library, 
to write tests for all stages of the compiler.
Multiple parts of the compiler and outputted code will
need to be benchmarked. I used a benchmarking library called
Criterion \cite{criterion} for micro-benchmarks, running a function
multiple times and finding the mean runtime with accuracy bounds.
Accuracy bound were useful in writing the evaluation section.

\section{General}

I decided to write the compiler in Haskell, this choice was made for two reasons: 
\begin{itemize}
  \item Learning to program in Haskell, would help me better understand the Haskell language
    and therefore help in making more informed design decisions when implementing Haskell features.

  \item Functional programming languages lend themselves well to implementing a compiler. This is due to the fact that the compiler will just be operating
    on and modifying syntax trees, which functional languages excel at.

\end{itemize}

  The first preparation was to read fully and make notes on
  the Haskell report, this influenced design decisions from the start:
  noting the need for kind inference, I then decided to 
  place this process in the desugaring stage of the pipeline. 
  Then after reading the JVM 
  specification \cite{jvm-spec8} I produced a
  document which my supervisor and I discussed and agreed
  upon the fact this would be a simple and efficient implementation
  of the Haskell constructs. The document is included in 
  Appendix~\ref{appendix:bytecodetranslationdoc}. 
  The JVM is a good backend choice to compile Haskell to for two
  reasons, first it allows access to the full feature set
  of JVM tool as mentioned in the introduction and secondly
  the JVM itself provides garbage collection and 
  a more simple backend target (when compared to machine assembly).
  This allowed me to implement a whole compiler in the timescale given.
  I will now discuss the decisions made for each stage of the 
  pipeline before beginning the implementation.

\section{Lexing and Parsing}

I chose to use a lexing generator called Alex \cite{alex-lib} and a parsing generator called Happy \cite{happy-lib}.
These two libraries are very similar to Lex \cite{lex-lib} and Yacc
\cite{yacc-lib}, but target Haskell.
They both have manuals on their respective webpages which document 
implementing a simple lexer and parser respectively then describing
more complex examples.
I also looked at the Haskell 98 report \cite[\textsection2.,3.]{haskell98-spec}
to figure out what syntax JVHC will support and therefore, what 
should be supported by my lexer and parser implementations.
The implementation will not support all of the Haskell 98 
specification meaning that the language accepted by the lexer and 
parser could be simplified some what when compared to the full
grammar defined in the report.

Haskell 98 is a whitespace sensitive language meaning that the parser must use a
context sensitive grammar to correctly parser a program:
\begin{HaskellLst}
f x = case x of 
  True  -> 1
  False -> 0
\end{HaskellLst}
Notice the double space before the \texttt{True} and \texttt{False}, this in conjunction with the
newline at the \texttt{1} is syntax for a cases statement. The Haskell 98 report
says how the parser should first insert \texttt{;},  \texttt{\{} and \texttt{\}} symbols at the 
relevant places:
\begin{HaskellLst}
f x = case of {
  True  -> 1;
  False -> 0
}
\end{HaskellLst}
This turns the whitespace sensitive language into a whitespace insensitive language. I chose to 
not implement a parser which would perform this translation since this would add extra complexity to the
implementation, I believe the parsing stage is less interesting than other stages in the pipeline.
This means my parser will accept programs defined using the whitespace insensitive 
syntax presented above.


\section{Desugaring}

The interesting part of the desugaring stage is 
checking that all custom data types which are declared at the top of a
file make sense in the context of Haskell. I will now explain this
problem in more detail.

Haskell has the notion of kinds, this is a form of type constructor. This will
formalize the notion of checking all custom data types declared.
The syntax of kinds is
\[ \mathtt{K} ::= \mathtt{*}\ |\ \mathtt{K}\  |\ \mathtt{K}_1 \rightarrow \mathtt{K}_2 \] 
Where \texttt{*} is called type and will match another
\texttt{*}, then \texttt{K} is a kind variable, and $\mathtt{K} \rightarrow \mathtt{K}$ is a type to type constructor.
Haskell also imposes another constraint that any type variable \texttt{K} will
be default to \texttt{*} (after kind inference) so as to give 
unique kind to each declaration without any kind variables.
The Haskell 98 report has a section \cite[4.6]{haskell98-spec} called Kind inference.
The example is drawn from that report:
\begin{HaskellLst}
data App f a = A (f a)
\end{HaskellLst}
This example will generate two constraints \texttt{a} has kind \texttt{K} and \texttt{f} 
has kind $\mathtt{K'} \rightarrow \mathtt{K}$. This \texttt{App} declaration will have kind  
kind $(\mathtt{*}\rightarrow\mathtt{*})\rightarrow\mathtt{*}\rightarrow
\mathtt{*}$.

The other part of the desugaring stage is to generate a syntax tree which can be used by the type checker
with appropriately simple syntactic constructs, reducing the number of cases that need
be considered in the type inference stage.

\section{Type Inference}

Type inference is the problem:
\begin{displayquote}
Given an expression $e$ find the type $\tau$ for the expression or return an error if not possible to
find a type for $e$.
\end{displayquote}
The solution to this section came mostly from a paper called `Typing Haskell in Haskell' \cite{thih-paper}. 
This provides a very clear outline on how to implement type inference. 
There are a few modifications that will be made to the approach.
Firstly I will remove any reference of type classes since they are not to be implemented in JVHC.
Secondly in the type checker a new expression data type called \texttt{CoreExpr} 
is generated which contains the type information discovered when running the type checker.

\texttt{CoreExpr} is a datatype with seven simple constructs which can express any expression in Haskell.
This data type was inspired by GHC \cite{typedcorelink}. This is a System F like typed lambda calculus with
built in support for constant values and data type deconstructors, in the form of \texttt{case} expressions.

\section{Inlining}

This section was inspired by a paper called ``Secrets of the 
Glasgow Haskell Compiler inliner'' \cite{ghc-inliner}. This outlines the basic idea of functional 
inlining. The idea is given an expression:
\begin{HaskellLst}
let f x = x + 2 in 
  (f 2) + 4
\end{HaskellLst}
it could be reduced to
\begin{HaskellLst}
(2 + 2) + 4
\end{HaskellLst}
The transformation removes the overhead of allocating a new stack frame to call 
the function \texttt{f} at runtime.

The core calculus is System F but with recursive functions, 
this means that \textit{strong normalization}, 
need not hold, therefore care must be taken to make sure the inlining terminates.

\begin{displayquote} Strong normalization says that a well typed expression in System F will
reduce to a term with no $\beta$-redexes in a finite number of steps.
\end{displayquote}

\section{Code Generation}

I familiarized myself with the JVM8 specification \cite{jvm-spec8}, and finally wrote a document in Appendix~\ref{appendix:bytecodetranslationdoc} 
about how I would implement code generation, 
at a high level before starting to write any code.

To generate bytecode I will use the library Codec-JVM \cite{codec-jvm-link}. This library 
required me to specify the bytecode and the library would handle formatting the bytecode into
the \texttt{.class} file format accepted by the JVM as defined in the JVM specification. 




\end{document}
